{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Association Rules and Colaborative Filtering are popular methods in marketing for cross-selling \n",
    "products associated with an item that a consumer is cosidering. \n",
    "\n",
    "Association rule discovery in marketing is termed “market basket analysis” and is aimed at discovering \n",
    "which groups of products tend to be purchased together.\n",
    "\n",
    "In collaborative filtering, the goal is to provide personalized recommendations that leverage \n",
    "user-level information. User-based collaborative filtering starts with a user, then finds users \n",
    "who have purchased a similar set of items or ranked items in a similar fashion, and makes a \n",
    "recommendation to the initial user based on what the similar users purchased or liked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Put simply, association rules, or affinity analysis, constitute a study of “what goes with what.”\n",
    "This method is also called market basket analysis because it originated with the study of customer\n",
    "transactions databases to determine dependencies between purchases of different items.\n",
    "\n",
    "For example, a medical researcher might want to learn what symptoms appear together. In law, word \n",
    "combinations that appear too often might indicate plagiarism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Apriori Algorithm\n",
    "\n",
    "\n",
    "he key idea of the algorithm is to begin by generating frequent itemsets with just one item \n",
    "(one-itemsets) and to recursively generate frequent itemsets with two items, then with three \n",
    "items, and so on, until we have generated frequent itemsets of all sizes.\n",
    "\n",
    "It is easy to generate frequent one-itemsets. All we need to do is to count, for each item, \n",
    "ow many transactions in the database include the item. These transaction counts are the supports \n",
    "for the one-itemsets. We drop one-itemsets that have support below the desired minimum support to\n",
    "create a list of the frequent one-itemsets.\n",
    "\n",
    "To generate frequent two-itemsets, we use the frequent one-itemsets. The reasoning is that if a \n",
    "certain one-itemset did not exceed the minimum support, any larger size itemset that includes \n",
    "it will not exceed the minimum support. In general, generating k-itemsets uses the frequent \n",
    "(k − 1)-itemsets that were generated in the preceding step. Each step requires a single run \n",
    "through the database, and therefore the Apriori algorithm is very fast even for a large number \n",
    "of unique items in a database.\n",
    "\n",
    "Confiedence and Support\n",
    "In addition to support, which we described earlier, there is another measure that expresses the \n",
    "degree of uncertainty about the if–then rule. This is known as the confidence2 of the rule. This \n",
    "measure compares the co-occurrence of the antecedent and consequent itemsets in the database to\n",
    "the occurrence of the antecedent itemsets. Confidence is defined as the ratio of the number of \n",
    "transactions that include all antecedent and consequent itemsets (namely, the support) to the \n",
    "number of transactions that include all the antecedent itemsets\n",
    "\n",
    "Lift Ratio\n",
    "A better way to judge the strength of an association rule is to compare the confidence of the \n",
    "ule with a benchmark value, where we assume that the occurrence of the consequent itemset in a \n",
    "transaction is independent of the occurrence of the antecedent for each rule. \n",
    "\n",
    "Leverage \n",
    "Leverage measures the deviation from independence. It ranges from − 1 to 1 and is 0 if the antecedent \n",
    "and consequent are independent. In a sales setting, leverage tells us how much more frequently the \n",
    "items are bought together compared to their independent sales. \n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Red  White  Blue  Orange  Green  Yellow\n",
      "Transaction                                         \n",
      "1              1      1     0       0      1       0\n",
      "2              0      1     0       1      0       0\n",
      "3              0      1     1       0      0       0\n",
      "4              1      1     0       1      0       0\n",
      "5              1      0     1       0      0       0\n",
      "6              0      1     1       0      0       0\n",
      "7              1      0     1       0      0       0\n",
      "8              1      1     1       0      1       0\n",
      "9              1      1     1       0      0       0\n",
      "10             0      0     0       0      0       1\n",
      "       antecedents   consequents  support  confidence      lift  leverage\n",
      "14    (Red, White)       (Green)      0.2         0.5  2.500000      0.12\n",
      "15         (Green)  (Red, White)      0.2         1.0  2.500000      0.12\n",
      "4          (Green)         (Red)      0.2         1.0  1.666667      0.08\n",
      "13  (White, Green)         (Red)      0.2         1.0  1.666667      0.08\n",
      "7         (Orange)       (White)      0.2         1.0  1.428571      0.06\n",
      "8          (Green)       (White)      0.2         1.0  1.428571      0.06\n"
     ]
    }
   ],
   "source": [
    "#Apriori Algorithm\n",
    "\n",
    "# Load and preprocess data set \n",
    "fp_df = pd.read_csv('DataMining/Faceplate.csv')\n",
    "fp_df.set_index('Transaction', inplace=True)\n",
    "print(fp_df)\n",
    "# create frequent itemsets\n",
    "itemsets = apriori(fp_df, min_support=0.2, use_colnames=True)\n",
    "# convert into rules\n",
    "rules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\n",
    "rules.sort_values(by=['lift'], ascending=False).head(6)\n",
    "print(rules.sort_values(by=['lift'], ascending=False)\n",
    "      .drop(columns=['antecedent support', 'consequent support', 'conviction'])\n",
    "      .head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      antecedents   consequents  support  confidence      lift  leverage\n",
      "5         (Green)  (Red, White)      0.2         1.0  2.500000      0.12\n",
      "0         (Green)         (Red)      0.2         1.0  1.666667      0.08\n",
      "4  (White, Green)         (Red)      0.2         1.0  1.666667      0.08\n",
      "1        (Orange)       (White)      0.2         1.0  1.428571      0.06\n",
      "2         (Green)       (White)      0.2         1.0  1.428571      0.06\n",
      "3    (Red, Green)       (White)      0.2         1.0  1.428571      0.06\n"
     ]
    }
   ],
   "source": [
    "#AAssociation Rules Output for Random Data\n",
    "\n",
    "# create frequent itemsets\n",
    "itemsets = apriori(fp_df, min_support=2/len(fp_df), use_colnames=True)\n",
    "# and convert into rules\n",
    "rules = association_rules(itemsets, metric='confidence', min_threshold=0.7)\n",
    "print(rules.sort_values(by=['lift'], ascending=False)\n",
    "      .drop(columns=['antecedent support', 'consequent support', 'conviction'])\n",
    "      .head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>(RefBks, YouthBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.05525</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>2.809917</td>\n",
       "      <td>0.035588</td>\n",
       "      <td>2.368750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>(DoItYBks, RefBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.09250</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>0.662162</td>\n",
       "      <td>2.736207</td>\n",
       "      <td>0.038865</td>\n",
       "      <td>2.243680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>(DoItYBks, YouthBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.10325</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.06700</td>\n",
       "      <td>0.648910</td>\n",
       "      <td>2.681448</td>\n",
       "      <td>0.042014</td>\n",
       "      <td>2.158993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>(RefBks, GeogBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.08175</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.05025</td>\n",
       "      <td>0.614679</td>\n",
       "      <td>2.539995</td>\n",
       "      <td>0.030467</td>\n",
       "      <td>1.967190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>(GeogBks, YouthBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.06325</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>2.501087</td>\n",
       "      <td>0.037961</td>\n",
       "      <td>1.920267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>(DoItYBks, GeogBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.06050</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>2.475248</td>\n",
       "      <td>0.036058</td>\n",
       "      <td>1.890321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>(ChildBks, GeogBks, CookBks)</td>\n",
       "      <td>(YouthBks)</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.06325</td>\n",
       "      <td>0.577626</td>\n",
       "      <td>2.424452</td>\n",
       "      <td>0.037162</td>\n",
       "      <td>1.803495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>(ChildBks, RefBks, CookBks)</td>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>0.10350</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>0.591787</td>\n",
       "      <td>2.323013</td>\n",
       "      <td>0.034883</td>\n",
       "      <td>1.825642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(DoItYBks, GeogBks)</td>\n",
       "      <td>(YouthBks)</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.05450</td>\n",
       "      <td>0.539604</td>\n",
       "      <td>2.264864</td>\n",
       "      <td>0.030437</td>\n",
       "      <td>1.654554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>(ChildBks, RefBks, CookBks)</td>\n",
       "      <td>(YouthBks)</td>\n",
       "      <td>0.10350</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.05525</td>\n",
       "      <td>0.533816</td>\n",
       "      <td>2.240573</td>\n",
       "      <td>0.030591</td>\n",
       "      <td>1.634013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>(ChildBks, CookBks, DoItYBks)</td>\n",
       "      <td>(YouthBks)</td>\n",
       "      <td>0.12775</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.06700</td>\n",
       "      <td>0.524462</td>\n",
       "      <td>2.201309</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>1.601869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>(ChildBks, YouthBks, CookBks)</td>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.06700</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>2.191691</td>\n",
       "      <td>0.036430</td>\n",
       "      <td>1.687358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(ChildBks, RefBks)</td>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>0.12825</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.07100</td>\n",
       "      <td>0.553606</td>\n",
       "      <td>2.173135</td>\n",
       "      <td>0.038328</td>\n",
       "      <td>1.669490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>(ChildBks, GeogBks, CookBks)</td>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.06050</td>\n",
       "      <td>0.552511</td>\n",
       "      <td>2.168838</td>\n",
       "      <td>0.032605</td>\n",
       "      <td>1.665406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(ChildBks, GeogBks)</td>\n",
       "      <td>(YouthBks)</td>\n",
       "      <td>0.14625</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.07550</td>\n",
       "      <td>0.516239</td>\n",
       "      <td>2.166797</td>\n",
       "      <td>0.040656</td>\n",
       "      <td>1.574642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(GeogBks, CookBks)</td>\n",
       "      <td>(YouthBks)</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.08025</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>2.155719</td>\n",
       "      <td>0.043023</td>\n",
       "      <td>1.566098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>(ChildBks, RefBks, YouthBks)</td>\n",
       "      <td>(CookBks)</td>\n",
       "      <td>0.06200</td>\n",
       "      <td>0.41550</td>\n",
       "      <td>0.05525</td>\n",
       "      <td>0.891129</td>\n",
       "      <td>2.144715</td>\n",
       "      <td>0.029489</td>\n",
       "      <td>5.368741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(ChildBks, YouthBks)</td>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>0.14750</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.08025</td>\n",
       "      <td>0.544068</td>\n",
       "      <td>2.135693</td>\n",
       "      <td>0.042674</td>\n",
       "      <td>1.634563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>(RefBks, CookBks)</td>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>0.13975</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.07450</td>\n",
       "      <td>0.533095</td>\n",
       "      <td>2.092619</td>\n",
       "      <td>0.038899</td>\n",
       "      <td>1.596148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(RefBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.20475</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.10350</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>2.088820</td>\n",
       "      <td>0.053950</td>\n",
       "      <td>1.532844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>(DoItYBks, RefBks, CookBks)</td>\n",
       "      <td>(ChildBks)</td>\n",
       "      <td>0.07450</td>\n",
       "      <td>0.39400</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>0.822148</td>\n",
       "      <td>2.086669</td>\n",
       "      <td>0.031897</td>\n",
       "      <td>3.407321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(YouthBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.503673</td>\n",
       "      <td>2.081292</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>1.527218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>(ChildBks, RefBks, DoItYBks)</td>\n",
       "      <td>(CookBks)</td>\n",
       "      <td>0.07100</td>\n",
       "      <td>0.41550</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>2.076236</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>4.256359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.12775</td>\n",
       "      <td>0.527893</td>\n",
       "      <td>2.072198</td>\n",
       "      <td>0.066101</td>\n",
       "      <td>1.578560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(DoItYBks)</td>\n",
       "      <td>(ChildBks, CookBks)</td>\n",
       "      <td>0.25475</td>\n",
       "      <td>0.24200</td>\n",
       "      <td>0.12775</td>\n",
       "      <td>0.501472</td>\n",
       "      <td>2.072198</td>\n",
       "      <td>0.066101</td>\n",
       "      <td>1.520476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      antecedents          consequents  antecedent support  \\\n",
       "64             (RefBks, YouthBks)  (ChildBks, CookBks)             0.08125   \n",
       "73             (DoItYBks, RefBks)  (ChildBks, CookBks)             0.09250   \n",
       "60           (DoItYBks, YouthBks)  (ChildBks, CookBks)             0.10325   \n",
       "80              (RefBks, GeogBks)  (ChildBks, CookBks)             0.08175   \n",
       "69            (GeogBks, YouthBks)  (ChildBks, CookBks)             0.10450   \n",
       "77            (DoItYBks, GeogBks)  (ChildBks, CookBks)             0.10100   \n",
       "66   (ChildBks, GeogBks, CookBks)           (YouthBks)             0.10950   \n",
       "71    (ChildBks, RefBks, CookBks)           (DoItYBks)             0.10350   \n",
       "47            (DoItYBks, GeogBks)           (YouthBks)             0.10100   \n",
       "62    (ChildBks, RefBks, CookBks)           (YouthBks)             0.10350   \n",
       "57  (ChildBks, CookBks, DoItYBks)           (YouthBks)             0.12775   \n",
       "58  (ChildBks, YouthBks, CookBks)           (DoItYBks)             0.12000   \n",
       "33             (ChildBks, RefBks)           (DoItYBks)             0.12825   \n",
       "75   (ChildBks, GeogBks, CookBks)           (DoItYBks)             0.10950   \n",
       "19            (ChildBks, GeogBks)           (YouthBks)             0.14625   \n",
       "46             (GeogBks, CookBks)           (YouthBks)             0.15625   \n",
       "61   (ChildBks, RefBks, YouthBks)            (CookBks)             0.06200   \n",
       "16           (ChildBks, YouthBks)           (DoItYBks)             0.14750   \n",
       "51              (RefBks, CookBks)           (DoItYBks)             0.13975   \n",
       "28                       (RefBks)  (ChildBks, CookBks)             0.20475   \n",
       "72    (DoItYBks, RefBks, CookBks)           (ChildBks)             0.07450   \n",
       "15                     (YouthBks)  (ChildBks, CookBks)             0.23825   \n",
       "70   (ChildBks, RefBks, DoItYBks)            (CookBks)             0.07100   \n",
       "23            (ChildBks, CookBks)           (DoItYBks)             0.24200   \n",
       "25                     (DoItYBks)  (ChildBks, CookBks)             0.25475   \n",
       "\n",
       "    consequent support  support  confidence      lift  leverage  conviction  \n",
       "64             0.24200  0.05525    0.680000  2.809917  0.035588    2.368750  \n",
       "73             0.24200  0.06125    0.662162  2.736207  0.038865    2.243680  \n",
       "60             0.24200  0.06700    0.648910  2.681448  0.042014    2.158993  \n",
       "80             0.24200  0.05025    0.614679  2.539995  0.030467    1.967190  \n",
       "69             0.24200  0.06325    0.605263  2.501087  0.037961    1.920267  \n",
       "77             0.24200  0.06050    0.599010  2.475248  0.036058    1.890321  \n",
       "66             0.23825  0.06325    0.577626  2.424452  0.037162    1.803495  \n",
       "71             0.25475  0.06125    0.591787  2.323013  0.034883    1.825642  \n",
       "47             0.23825  0.05450    0.539604  2.264864  0.030437    1.654554  \n",
       "62             0.23825  0.05525    0.533816  2.240573  0.030591    1.634013  \n",
       "57             0.23825  0.06700    0.524462  2.201309  0.036564    1.601869  \n",
       "58             0.25475  0.06700    0.558333  2.191691  0.036430    1.687358  \n",
       "33             0.25475  0.07100    0.553606  2.173135  0.038328    1.669490  \n",
       "75             0.25475  0.06050    0.552511  2.168838  0.032605    1.665406  \n",
       "19             0.23825  0.07550    0.516239  2.166797  0.040656    1.574642  \n",
       "46             0.23825  0.08025    0.513600  2.155719  0.043023    1.566098  \n",
       "61             0.41550  0.05525    0.891129  2.144715  0.029489    5.368741  \n",
       "16             0.25475  0.08025    0.544068  2.135693  0.042674    1.634563  \n",
       "51             0.25475  0.07450    0.533095  2.092619  0.038899    1.596148  \n",
       "28             0.24200  0.10350    0.505495  2.088820  0.053950    1.532844  \n",
       "72             0.39400  0.06125    0.822148  2.086669  0.031897    3.407321  \n",
       "15             0.24200  0.12000    0.503673  2.081292  0.062344    1.527218  \n",
       "70             0.41550  0.06125    0.862676  2.076236  0.031749    4.256359  \n",
       "23             0.25475  0.12775    0.527893  2.072198  0.066101    1.578560  \n",
       "25             0.24200  0.12775    0.501472  2.072198  0.066101    1.520476  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_books_df = pd.read_csv('DataMining/CharlesBookClub.csv')\n",
    "# create the binary incidence matrix\n",
    "ignore = ['Seq#', 'ID#', 'Gender', 'M', 'R', 'F', 'FirstPurch', 'Related Purchase',\n",
    "          'Mcode', 'Rcode', 'Fcode', 'Yes_Florence', 'No_Florence']\n",
    "count_books = all_books_df.drop(columns=ignore)\n",
    "count_books[count_books > 0] = 1\n",
    "# create frequent itemsets and rules\n",
    "itemsets = apriori(count_books, min_support=200/4000, use_colnames=True)\n",
    "rules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\n",
    "# Display 25 rules with highest lift\n",
    "rules.sort_values(by=['lift'], ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collaborative filtering is a popular technique used by such recommendation systems. The term \n",
    "collaborative filtering is based on the notions of identifying relevant items for a specific user\n",
    "from the very large set of items (“filtering”) by considering preferences of many users \n",
    "(“collaboration”).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colaborative Filtering\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "nratings = 5000\n",
    "randomData = pd.DataFrame({\n",
    "    'itemID': [random.randint(0,99) for _ in range(nratings)],\n",
    "    'userID': [random.randint(0,999) for _ in range(nratings)],\n",
    "    'rating': [random.randint(1,5) for _ in range(nratings)],\n",
    "})\n",
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    byUser = defaultdict(list)\n",
    "    for p in predictions:\n",
    "        byUser[p.uid].append(p)\n",
    "    \n",
    "    # For each user, reduce predictions to top-n\n",
    "    for uid, userPredictions in byUser.items():\n",
    "        byUser[uid] = heapq.nlargest(n, userPredictions, key=lambda p: p.est)\n",
    "    return byUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Top-3 recommended items for each user\n",
      "User 6\n",
      "  Item 6 (5.00)  Item 77 (2.50)  Item 60 (1.00)\n",
      "User 222\n",
      "  Item 77 (3.50)  Item 75 (2.78)\n",
      "User 424\n",
      "  Item 14 (3.50)  Item 45 (3.10)  Item 54 (2.34)\n",
      "User 87\n",
      "  Item 27 (3.00)  Item 54 (3.00)  Item 82 (3.00)  Item 32 (1.00)\n",
      "User 121\n",
      "  Item 98 (3.48)  Item 32 (2.83)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data set into the format required by the surprise package\n",
    "# The columns must correspond to user id, item id, and ratings (in that order)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(randomData[['userID', 'itemID', 'rating']], reader)\n",
    "# Split into training and test set\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=1)\n",
    "## User-based filtering\n",
    "# compute cosine similarity between users \n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "# predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "predictions = algo.test(testset) \n",
    "# Print the recommended items for each user\n",
    "top_n = get_top_n(predictions, n=4)\n",
    "print('Top-3 recommended items for each user')\n",
    "for uid, user_ratings in list(top_n.items())[:5]:\n",
    "    print('User {}'.format(uid))\n",
    "    for prediction in user_ratings:\n",
    "        print('  Item {0.iid} ({0.est:.2f})'.format(prediction), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(uid=383, iid=7, r_ui=None, est=2.3661840936304324, details={'actual_k': 4, 'was_impossible': False})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = data.build_full_trainset()\n",
    "sim_options = {'name': 'cosine', 'user_based': False}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "# Predict rating for user 383 and item 7\n",
    "algo.predict(383, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Associatuin Rules vs Colaborative Filtering \n",
    "\n",
    "1. Frequent itemsets vs. personalized recommendations: Association rules look for frequent item \n",
    "combinations and will provide recommendations only for those items. In contrast, collaborative \n",
    "filtering provides personalized recommendations for every item, thereby catering to users with\n",
    "unusual taste. In this sense, collaborative filtering is useful for capturing the “long tail” of\n",
    "user preferences, while association rules look for the “head.” \n",
    "\n",
    "2. Transactional data vs. user data: Association rules provide recommendations of items based on\n",
    "their co-purchase with other items in many transactions/baskets.\n",
    " In contrast, collaborative filtering provides recommendations of items based on their \n",
    " co-purchase or co-rating by even a small number of other users.\n",
    " \n",
    "3. Binary data and ratings data: Association rules treat items as binary data (1 = purchase, \n",
    "0 = nonpurchase), whereas collaborative filtering can operate on either binary data or on \n",
    "numerical ratings.\n",
    "\n",
    "4. Two or more items: In association rules, the antecedent and consequent can each include one or\n",
    "more items (e.g., IF milk THEN cookies and cornflakes). Hence, a recommendation might be a bundle\n",
    "of the item of interest with multiple items (“buy milk, cookies, and cornflakes and receive 10% \n",
    "discount’’). In contrast, in collaborative filtering, similarity is measured between pairs of items\n",
    "or pairs of users. A recommendation will therefore be either for a single item (the most popular \n",
    "item purchased by people like you, which you haven’t purchased), or for multiple single items \n",
    "which do not necessarily relate to each other (the top two most popular items purchased by people\n",
    "like you, which you haven’t purchased).\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
